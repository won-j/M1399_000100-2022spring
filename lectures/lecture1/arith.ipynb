{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dig into the internal representation of R objects\n",
    "library(lobstr)  # if not installed, use install.packages('lobstr')\n",
    "# For unsigned integers\n",
    "library(uint8) # devtools::install_github('coolbutuseless/uint8')\n",
    "# For bitstrings\n",
    "library(pryr)\n",
    "# For big integers\n",
    "library(gmp)\n",
    "# For single precision floating point numbers\n",
    "library(float)\n",
    "library(Rcpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer arithmetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units of computer storage\n",
    "\n",
    "Humans use decimal digits (why?)\\\n",
    "Computers use binary digits (why?)\n",
    "\n",
    "* *Bit* = binary digit (coined by statistician [John Tukey](https://en.wikipedia.org/wiki/Bit#History)).  \n",
    "* *byte* = 8 bits.\n",
    "* KB = kilobyte = $10^3$ bytes; KiB = kibibyte = $2^{10}$ bytes.  \n",
    "* MB = megabyte = $10^6$ bytes; MiB = mebibyte = $2^{20}$ bytes.\n",
    "* GB = gigabyte = $10^9$ bytes. Typical RAM size.  \n",
    "* TB = terabyte = $10^{12}$ bytes. Typical hard drive size. Size of NYSE each trading session.    \n",
    "* PB = petabyte = $10^{15}$ bytes.  \n",
    "* EB = exabyte = $10^{18}$ bytes. Size of all healthcare data in 2011 is ~150 EB.    \n",
    "* ZB = zetabyte = $10^{21}$ bytes. \n",
    "\n",
    "R function `lobstr::obj_size()` shows the amount of memory (in bytes) used by an object. (This is a better version of the built-in `utils::object.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- 100\n",
    "lobstr::obj_size(x)\n",
    "y <-c(20, 30)\n",
    "lobstr::obj_size(y)\n",
    "z <- as.matrix(runif(100 * 100), nrow=100)  # 100 x 100 random matrix\n",
    "lobstr::obj_size(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all variables in workspace and their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(ls(), function(z, env=parent.env(environment())) obj_size(get(z, envir=env)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage of Characters\n",
    "\n",
    "* Plain text files are stored in the form of characters: `.jl`, `.r`, `.c`, `.cpp`, `.ipynb`, `.html`, `.tex`, ...  \n",
    "* ASCII (American Code for Information Interchange): 7 bits, only $2^7=128$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 0, 1, ..., 127 and corresponding ascii character\n",
    "sapply(0:127, intToUtf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extended ASCII: 8 bits, $2^8=256$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 128, 129, ..., 255 and corresponding extended ascii character\n",
    "sapply(128:255, intToUtf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unicode: UTF-8, UTF-16 and UTF-32 support many more characters including foreign characters; last 7 digits conform to ASCII. \n",
    "\n",
    "* [UTF-8](https://en.wikipedia.org/wiki/UTF-8) is the current dominant character encoding on internet.  \n",
    "\n",
    "<img src=\"./unicode.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Source: [Google Blog](https://googleblog.blogspot.com/2012/02/unicode-over-60-percent-of-web.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st <-  \"\\uD1B5\\uACC4\\uACC4\\uC0B0\"\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integers: fixed-point number system\n",
    "\n",
    "* Fixed-point number system is a computer model for integers $\\mathbb{Z}$. \n",
    "    - **Remember** that computer memory is finite whereas the cardinality of $\\mathbb{Z}$ is (countably) infinite.\n",
    "    - *Any* representation of numbers in computer *has to be* an approximation.\n",
    "\n",
    "* The number $M$ of bits and method of representing negative numbers vary from system to system. \n",
    "    - The `integer` type in R has $M=32$ (packages such as ‘bit64’support 64 bit integers). \n",
    "        + <https://www.r-bloggers.com/r-in-a-64-bit-world/>\n",
    "    - C has (`unsigned`) `char`, `int`, `short`, `long` (and `long long`), whose sizes depend on the machine.\n",
    "    - Matlab has `(u)int8`, `(u)int16`, `(u)int32`, `(u)int64`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsigned integers\n",
    "\n",
    "* Model for $\\mathbb{N} \\cup \\{0\\}$.\n",
    "* For unsigned integers, the range is $[0,2^M-1]$.\n",
    "* R does not support unsigned integers natively (will see `unit8` package later). In most other languages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type        | Min           | Max  |\n",
    "| ------------- |:-------------:|:------|\n",
    "|UInt8\t|0\t|255|\n",
    "|UInt16\t|0\t|65535|\n",
    "|UInt32\t|0\t|4294967295|\n",
    "|UInt64\t|0\t|18446744073709551615|\n",
    "|UInt128\t|0\t|340282366920938463463374607431768211455|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signed integers\n",
    "\n",
    "* Model of $\\mathbb{Z}$. Can do subtraction.\n",
    "\n",
    "* First bit (\"most significant bit\" or MSB) is the sign bit.  \n",
    "    - `0` for nonnegative numbers\n",
    "    - `1` for negative numbers  \n",
    "    \n",
    "* **Two's complement representation** for negative numbers. \n",
    "    - Set the sign bit to 1  \n",
    "    - Negate (`0`->`1`, `1`->`0`) the remaining bits\n",
    "    - Add to `1` to the result  \n",
    "    - Two's complement representation of a negative integer $x$ is the same as the unsigned integer $2^M - x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(5L)  # postfix `L` means integer in R\n",
    "pryr::bits(5L)\n",
    "pryr::bits(-5L)\n",
    "pryr::bits(2L * 5L) # shift bits of 5 to the left (why?)\n",
    "pryr::bits(2L * -5L); # shift bits of -5 to left "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two's complement representation respects modular arithmetic nicely.  \n",
    "    Addition of any two signed integers are just bitwise addition, possibly modulo $2^M$\n",
    "    - $M=4$ case:\n",
    "    \n",
    "<img src=\"http://users.dickinson.edu/~braught/courses/cs251f02/classes/images/twosCompWheel.png\" width=\"400\" align=\"center\"/>    \n",
    "\n",
    "Source: [Signed Binary Numbers, Subtraction and Overflow](http://users.dickinson.edu/~braught/courses/cs251f02/classes/notes07.html) by Grant Braught"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Range** of representable integers by $M$-bit **signed integer** is $[-2^{M-1},2^{M-1}-1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine$integer.max  # R uses 32-bit integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most other languages,\n",
    "\n",
    "| Type        | Min           | Max  |\n",
    "| ------------- |--------------:|:------|\n",
    "|Int8\t|\t-128|127|\n",
    "|Int16\t|-32768\t|32767|\n",
    "|Int32\t|-2147483648\t|2147483647|\n",
    "|Int64\t|-9223372036854775808\t|9223372036854775807|\n",
    "|Int128\t|-170141183460469231731687303715884105728\t|170141183460469231731687303715884105727|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow for integer arithmetic\n",
    "\n",
    "R reports `NA` for integer overflow and underflow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The largest integer R can hold\n",
    ".Machine$integer.max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M <- 32\n",
    "big <- 2^(M-1) - 1\n",
    "as.integer(big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine$integer.max + 1L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unit8` outputs the result according to modular arithmetic. So does C and Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uint8::as.uint8(255) + uint8::as.uint8(1)\n",
    "uint8::as.uint8(250) + uint8::as.uint8(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `gmp` supports big integers with arbitrary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp::as.bigz(.Machine$integer.max ) + gmp::as.bigz(1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real numbers: floating-point number system\n",
    "\n",
    "Floating-point number system is a computer model for the real line $\\mathbb{R}$.\n",
    "\n",
    "* Most computer systems adopt the [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point), established in 1985, for floating-point arithmetics.  \n",
    "For the history, see an [interview with William Kahan](http://www.cs.berkeley.edu/~wkahan/ieee754status/754story.html).\n",
    "\n",
    "* In the scientific notation, a real number is represented as\n",
    "$$\n",
    "\\pm d_1.d_2d_3 \\cdots d_p \\times b^e, \\quad 0 \\le d_i < b.\n",
    "$$\n",
    "Humans use the _base_ $b=10$ and _digits_ $d_i=0, 1, \\dotsc, 9$.\\\n",
    "    In computer, the base is $b=2$ and the digits $d_i$ are 0 or 1.\n",
    "\n",
    "* **Normalized** vs **denormalized** numbers. For example, decimal number 18 is\n",
    "$$ +1.0010 \\times 2^4 \\quad (\\text{normalized})$$\n",
    "or, equivalently,\n",
    "$$ +0.1001 \\times 2^5 \\quad (\\text{denormalized}).$$\n",
    "\n",
    "* In the floating-number system, computer stores \n",
    "    - sign bit  \n",
    "    - the _fraction_ (or _mantissa_, or _significand_) of the **normalized** representation\n",
    "    - the actual exponent _plus_ a bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* R supports *double precesion* floating point numbers (see below) via `double`. \n",
    "\n",
    "* C supports floating point types `float` and `double`, where in most systems `float` corresponds to single precision while `double` corresponds to double precision.\n",
    "\n",
    "* Julia provides `Float16` (half precision, implemented in software using `Float32`), `Float32` (single precision), `Float64` (double precision), and `BigFloat` (arbitrary precision).\n",
    "\n",
    "> R has no single precision data type. All real numbers are stored in double precision format. The functions `as.single` and `single` are identical to `as.double` and `double` except they set the attribute `Csingle` that is used in the `.C` and `.Fortran` interface, and they are intended only to be used in that context. [R Documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/double.html)\n",
    "\n",
    "* For ease of exposition, we begin with half precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half precision (Float16)\n",
    "\n",
    "<img src=\"./half-precision-numbers.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Half-precision_floating-point_format>\n",
    "    \n",
    "- In Julia, `Float16` is the type for half precision numbers.\n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 10 significand bits (**fraction**=**mantissa**), hence $p=11$ (why?)\n",
    "\n",
    "- 5 exponent bits: $e_{\\max}=15$, $e_{\\min}=-14$, **bias**=15 = $01111_2$ for encoding:\n",
    "    + $e_{\\min} = \\mathbf{00001_2} - 01111_2 = -14_{10}$\n",
    "    + $e_{\\max} = \\mathbf{11110_2} - 01111_2 = 15_{10}$\n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 4}$ in decimal because $\\log_{10} (2^{15}) \\approx 4$.  \n",
    "\n",
    "- **Precision**: $\\log_{10}2^{11} \\approx 3.311$ decimal digits. \n",
    "\n",
    "$$\n",
    "(value) = (-1)^{b_{15}}\\times 2^{(\\sum_{j=1}^5 b_{15-j}2^{5-j}) - 15} \\times \\left( 1 + \\sum_{i=1}^{10}\\frac{b_{10-i}}{2^i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Julia\n",
    "# This is Julia\n",
    "println(\"Half precision:\")\n",
    "@show bitstring(Float16(5)) # 5 in half precision\n",
    "@show bitstring(Float16(-5)); # -5 in half precision\n",
    "```\n",
    "\n",
    "```\n",
    "Half precision:\n",
    "bitstring(Float16(5)) = \"0100010100000000\"\n",
    "bitstring(Float16(-5)) = \"1100010100000000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single precision (Float32, or `float`)\n",
    "\n",
    "<img src=\"./single-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Single-precision_floating-point_format>\n",
    "\n",
    "- In C, `float` is the type for single precision numbers for most systems.\n",
    "- In Julia, `Float32` is the type for single precision numbers.  \n",
    "- In R, single precision is not supported natively. We use the following workaround:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework: figure out how this C++ code works\n",
    "Rcpp::cppFunction('int float32bin(double x) {\n",
    "    float flx = (float) x; \n",
    "    unsigned int binx = *((unsigned int*)&flx); \n",
    "    return binx; \n",
    "}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSB is the sign bit.  \n",
    "\n",
    "- 23 significand bits ($p=24$).  \n",
    "\n",
    "- 8 exponent bits: $e_{\\max}=127$, $e_{\\min}=-126$, **bias**=127.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 38}$ in decimal because $\\log_{10} (2^{127}) \\approx 38$.\n",
    "\n",
    "- **precision**: $\\log_{10}(2^{24}) \\approx 7.225$ decimal digits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message(\"Single precision:\")\n",
    "pryr::bits(float32bin(5)) # 5 in single precision\n",
    "pryr::bits(float32bin(-5)) # -5 in single precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double precision (Float64, or `double`)\n",
    "\n",
    "<img src=\"./double-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Double-precision_floating-point_format>\n",
    "\n",
    "- Double precision (64 bits = 8 bytes) numbers are the dominant data type in scientific computing.\n",
    "\n",
    "- In C, `double` is the type for double precision numbers for most systems. It is the **default** type for `numeric` values.\n",
    "\n",
    "- In Julia, `Float64` is the type for double precision numbers.    \n",
    "\n",
    "- In R, `double` is the type for double precision numbers.    \n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 52 significand bits ($p=15$).\n",
    "\n",
    "- 11 exponent bits: $e_{\\max}=1023$, $e_{\\min}=-1022$, **bias**=1023.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 308}$ in decimal because $\\log_{10} (2^{1023}) \\approx 308$.  \n",
    "\n",
    "- **precision** to the $\\log_{10}(2^{53}) \\approx 15.95$ decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message(\"Double precision:\")\n",
    "pryr::bits(5)    # 5 in double precision\n",
    "pryr::bits(-5)   # -5 in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special floating-point numbers\n",
    "\n",
    "- Exponent $e_{\\max}+1$ plus a zero mantissa means $\\pm \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(Inf)    # Inf in double precision\n",
    "pryr::bits(-Inf)   # -Inf in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\max}+1$ plus a nonzero mantissa means `NaN`. `NaN` could be produced from `0 / 0`, `0 * Inf`, ...  \n",
    "\n",
    "- In general `NaN ≠ NaN` bitwise. Test whether a number is `NaN` by `is.nan` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0 / 0)  # NaN\n",
    "pryr::bits(0 * Inf) # NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a zero mantissa represents the real number 0 (\"exact zero\").  \n",
    "    + Why do we need an exact zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0.0)  # 0 in double precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a nonzero mantissa are for numbers less than $b^{e_{\\min}}$.  \n",
    "    Numbers are _denormalized_ in the range $(0,b^{e_{\\min}})$ -- **gradual underflow**. \n",
    "- For example, in half-precision, $e_{\\min}=-14$ but $2^{-24}$ is represented by $0.0000000001_2 \\times 2^{-14}$.\n",
    "- In single precision, $e_{\\min}=-126$ but $2^{-149}$ is represented by $0.00000000000000000000001_2 \\times 2^{-126}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2^(-126)  # emin=-126\n",
    "pryr::bits(float32bin(2^(-126)))\n",
    "2^(-149)  # denormalized\n",
    "pryr::bits(float32bin(2^(-149)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding\n",
    "\n",
    "* Rounding is necessary whenever a number has more than $p$ significand bits. Most computer systems use the default IEEE 754 _round to nearest_ mode (also called _ties to even_ mode). For example, the number 1/10 cannot be represented accurately as a (binary) floating point number:\n",
    "$$ 0.1 = 1.10011001\\dotsc_2 \\times 2^{-4} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0.1)  # double precision \n",
    "pryr::bits(float32bin(0.1)) # single precision, 1001 gets rounded to 101(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Rounding (more fundamentally, finite precision) incurs errors in floating porint computation. If a real number $x$ is represented by a floating point number $[x]$, then\n",
    "\n",
    "* Absolute error\n",
    "$$\n",
    "    | [x] - x |\n",
    "$$\n",
    "\n",
    "* Relative error\n",
    "$$\n",
    "    \\frac{| [x] - x |}{|x|}\n",
    "$$\n",
    "(if $x \\neq 0$).\n",
    "\n",
    "Of course, we want to ensure that the error after a computation is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine epsilons\n",
    "\n",
    "- Floating-point numbers do not occur uniformly over the real number line\n",
    "    <img src=\"http://www.volkerschatz.com/science/pics/fltscale-wh.png\" width=\"700\" align=\"center\"/>\n",
    "    \n",
    "    Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)\n",
    "    \n",
    "- Same number of representible numbers in $(2^i, 2^{i+1}]$ as in $(2^{i+1}, 2^{i+2}]$. Within an interval, they are uniformly distributed.\n",
    "    \n",
    "- **Machine epsilons** are the spacings of numbers around 1: \n",
    "    + $\\epsilon_{\\max}$ = (smallest positive floating point number that added to 1 will give a result different from 1) = $\\frac{1}{2^p} + \\frac{1}{2^{2p-1}}$\n",
    "    + $\\epsilon_{\\min}$ = (smallest positive floating point number that subtracted from 1 will give a result different from 1) = $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$.\n",
    "    \n",
    "    <img src=\"./machine_epsilons.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "Source: *Computational Statistics*, James Gentle, Springer, New York, 2009.\n",
    "    \n",
    "- \"*Round to nearest, ties to even*\" rule: rounds to the nearest value; if the number falls midway, it is rounded to the nearest value with an even least significant digit (default for IEEE 754 binary floating point numbers)\n",
    "\n",
    "- Any real number in the interval $\\left[1 - \\frac{1}{2^{p+1}}, 1 + \\frac{1}{2^p}\\right]$ is represented by a floating point number $1 = 1.00\\dotsc 0_2 \\times 2^0$.\n",
    "\n",
    "- Adding $\\frac{1}{2^p}$ to 1 won't reach the next representable floating point number  $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$. Hence $\\epsilon_{\\max} > \\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$.\n",
    "\n",
    "- Adding the floating point number next to $\\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$ to 1 WILL result in $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$, hence $\\epsilon_{\\max} = 1.00\\dotsc 01_2 \\times 2^{-p} = \\frac{1}{2^p} + \\frac{1}{2^{p+p-1}}$.\n",
    "\n",
    "- Subtracting $\\frac{1}{2^{p+1}}$ from 1 results in $1-\\frac{1}{2^{p+1}} = \\frac{1}{2} + \\frac{1}{2^2} + \\dotsb + \\frac{1}{2^p} + \\frac{1}{2^{p+1}}$, which is represented by the floating point number $1.00\\dotsc 0_2 \\times 2^{0} = 1$ by the \"ties to even\" rule. Hence $\\epsilon_{\\min} > \\frac{1}{2^{p+1}}$.\n",
    "\n",
    "- The smallest positive floating point number larger than $\\frac{1}{2^{p+1}}$ is $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}=1.00\\dotsc 1_2 \\times 2^{-p-1}$. Thus $\\epsilon_{\\min} = \\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine precision\n",
    "\n",
    "* Machine epsilon is often called the machine precision.\n",
    "\n",
    "* If a positive real number $x \\in \\mathbb{R}$ is represented by $[x]$ in the floating point arithmetic, then \n",
    "$$\n",
    "    [x] = \\left( 1 + \\sum_{i=1}^{p-1}\\frac{b_{i+1}}{2^i}\\right) \\times 2^e.\n",
    "$$\n",
    "Thus $x - \\frac{2^e}{2^p} < [x] \\le x + \\frac{2^e}{2^p}$, \n",
    "and\n",
    "$$\n",
    "    \\begin{split}\n",
    "    \\frac{| x - [x] |}{|x|} &\\le \\frac{2^e}{2^p|x|} \\le \\frac{2^e}{2^p}\\frac{1}{[x]-2^e/2^p} \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e(1-1/2^p)}  \\quad (\\because [x] \\ge 2^e) \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e}(1 + \\frac{1}{2^{p-1}}) \\\\\n",
    "                            &= \\frac{1}{2^p} + \\frac{1}{2^{2p-1}} = \\epsilon_{\\max}.\n",
    "    \\end{split}\n",
    "$$\n",
    "That is, the **relative error** of the floating point representation $[x]$ of real number $x$ is bounded by $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(digits=20)\n",
    "\n",
    "print(2^(-53) + 2^(-105))   # epsilon_max for double\n",
    "print(1.0 + 2^(-53))\n",
    "print(1.0 + (2^(-53) + 2^(-105)))\n",
    "print(1.0 + 2^(-53) + 2^(-105))  # why is the result?  See \"Catastrophic cancellation\"\n",
    "\n",
    "\n",
    "print(as.double(float::fl(2^(-24) + 2^(-47))))  # epsilon_max for float\n",
    "print(as.double(float::fl(1.0) + float::fl(2^(-24))))\n",
    "print(as.double(float::fl(1.0) + float::fl(2^(-24) + 2^(-47))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/2^54 + 1/2^106)  # epsilon_min for double\n",
    "print(1 - (1/2^54 + 1/2^106))\n",
    "pryr::bits(1.0)\n",
    "pryr::bits(1 - (1/2^54 + 1/2^106))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In R, the variable `.Machine` contains numerical characteristics of the machine. `double.neg.eps` is our $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow of floating-point number\n",
    "\n",
    "* For double precision, the range is $\\pm 10^{\\pm 308}$. In most situations, underflow (magnitude of result is less than $10^{-308}$) is preferred over overflow (magnitude of result is larger than $10^{+308}$). Overflow produces $\\pm \\inf$. Underflow yields zeros or subnormal numbers. \n",
    "\n",
    "* E.g., the logit link function is\n",
    "$$p = \\frac{\\exp (x^T \\beta)}{1 + \\exp (x^T \\beta)} = \\frac{1}{1+\\exp(- x^T \\beta)}.$$\n",
    "The former expression can easily lead to `Inf / Inf = NaN`, while the latter expression leads to gradual underflow.\n",
    "\n",
    "* `.Machine$double.xmax` and `.Machine$double.xmin` functions gives largest and smallest _non-subnormal_ number represented by the given floating point type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catastrophic cancellation\n",
    "\n",
    "* **Scenario 1**: Addition or subtraction of two numbers of widely different magnitudes: $a+b$ or $a-b$ where $a \\gg b$ or $a \\ll b$. We lose the precision in the number of smaller magnitude. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxx ... \\times 2^{30} \\\\  \n",
    "    b &=& y.yyy... \\times 2^{-30}\n",
    "\\end{eqnarray*}$$\n",
    "What happens when computer calculates $a+b$? We get $a+b=a$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a <- 2.0^30)\n",
    "(b <- 2.0^-30)\n",
    "(a + b == a)\n",
    "pryr::bits(a)\n",
    "pryr::bits(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: suppose we want to compute $x + y$, $x, y > 0$. Let the relative error of $x$ and $y$ be\n",
    "$$\n",
    "\\delta_x = \\frac{[x] - x}{x},\n",
    "\\quad\n",
    "\\delta_y = \\frac{[y] - y}{y}\n",
    ".\n",
    "$$\n",
    "What the computer actually calculates is $[x] + [y]$, which in turn is represented by $[ [x] + [y] ]$. The relative error of this representation is\n",
    "$$\n",
    "\\delta_{\\text{sum}} = \\frac{[[x]+[y]] - ([x]+[y])}{[x]+[y]}\n",
    ".\n",
    "$$\n",
    "Recall that $|\\delta_x|, |\\delta_y|, |\\delta_{\\text{sum}}| \\le \\epsilon_{\\max}$.\n",
    "\n",
    "We want to find a bound of the relative error of $[[x]+[y]]$ with respect to $x+y$.\n",
    "Since $|[x]+[y]| = |x(1+\\delta_x) + y(1+\\delta_y)| \\le |x+y|(1+\\epsilon_{\\max})$, we have\n",
    "$$\n",
    "\\begin{split}\n",
    "| [[x]+[y]-(x+y) | &= | [[x]+[y]] - [x] - [y] + [x] - x + [y] - y | \\\\\n",
    "                   &\\le | [[x]+[y]] - [x] - [y] | +  | [x] - x | + | [y] - y | \\\\\n",
    "                   &\\le |\\delta_{\\text{sum}}([x]+[y])| + |\\delta_x x| + |\\delta_y y| \\\\\n",
    "                   &\\le \\epsilon_{\\max}(x+y)(1+\\epsilon_{\\max}) + \\epsilon_{\\max}x + \\epsilon_{\\max}y \\\\\n",
    "                   &\\approx 2\\epsilon_{\\max}(x+y)\n",
    "\\end{split}\n",
    "$$\n",
    "because $\\epsilon_{\\max}^2 \\approx 0$. Thus\n",
    "$$\n",
    "\\frac{| [[x]+[y]-(x+y) |}{|x+y|} \\le 2\\epsilon_{\\max}\n",
    "$$\n",
    "approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scenario 2**: Subtraction of two nearly equal numbers eliminates significant digits.  $a-b$ where $a \\approx b$. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxxxxxxxxx1ssss  \\\\\n",
    "    b &=& x.xxxxxxxxxx0tttt\n",
    "\\end{eqnarray*}$$\n",
    "The result is $1.vvvvu...u$ where $u$ are unassigned digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olddigits <- options('digits')$digits\n",
    "options(digits=20)\n",
    "\n",
    "a <- float::fl(1.2345678) # rounding\n",
    "pryr::bits(float32bin(as.double(a))) # rounding\n",
    "b <- float::fl(1.2345677)\n",
    "pryr::bits(float32bin(as.double(b)))\n",
    "print(as.double(a - b)) # correct result should be 1f-7\n",
    "pryr::bits(float32bin(as.double(a - b)))   # must be 1.0000...0 x 2^(-23)\n",
    "print(1/2^23)\n",
    "\n",
    "options(digits=olddigits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Let\n",
    "$$\n",
    "[x] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{1}{2^{p-1}},\n",
    "\\quad\n",
    "[y] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{0}{2^{p-1}}\n",
    ".\n",
    "$$\n",
    "\n",
    "* $[x]-[y] = \\frac{1}{2^{p-1}} = [[x]-[y]]$.\n",
    "\n",
    "* The true difference $x-y$ may lie anywhere in $(0, \\frac{1}{2^{p-2}}+\\frac{1}{2^{2p}}]$.\n",
    "\n",
    "* Relative error \n",
    "$$\n",
    "\\frac{|x-y -[[x]-[y]]|}{|x-y|}\n",
    "$$\n",
    "is unbounded -- no guarantee of any significant digit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implications for numerical computation\n",
    "    - Rule 1: add small numbers together before adding larger ones  \n",
    "    - Rule 2: add numbers of like magnitude together (paring). When all numbers are of same sign and similar magnitude, add in pairs so each stage the summands are of similar magnitude  \n",
    "    - Rule 3: avoid substraction of two numbers that are nearly equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic laws\n",
    "\n",
    "Floating-point numbers may violate many algebraic laws we are familiar with, such associative and distributive laws. See the example in the Machine Epsilon section and HW1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coditioning\n",
    "\n",
    "Condiser solving a linear system $Ax=b$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "whose solution is $(x_1, x_2) = (1.000, 1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A <- matrix(c(1.0, 0.667, 0.5, 0.333), nrow=2)\n",
    "b <- c(1.5, 1.0)\n",
    "solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we *perturb* $b$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 0.999 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then the solution changes to $(x_1, x_2) = (0.000, 3.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 <- c(1.5, 0.999)\n",
    "solve(A, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead perturb $A$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.334 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then this time the solution changes to $(x_1, x_2) = (2.000, -1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 <- matrix(c(1.0, 0.667, 0.5, 0.334), nrow=2)\n",
    "solve(A1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, an input perturbation of order $10^{-3}$ yield an output perturbation of order $10^0$. Thats 3 orders of magnutides of relative change!\n",
    "\n",
    "Floating point representation $[x]$ of a real number $x$ in a digital computer may introduce such input perturbation easily. The perturbation of output of a problem with respect to the input is called *conditioning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abstractly, a *problem* can be viewed as function $f: X \\to Y$ where $X$ is a normed vector space of data and $Y$ is a normed vector space of solutions.\n",
    "    - The problem of solving $Ax=b$ for fixed $b$ is $f: A \\mapsto A^{-1}b$ with $X=\\{M\\in\\mathbb{R}^{n\\times n}: M \\text{ is invertible} \\}$ and $Y = \\mathbb{R}^n$.\n",
    "    - The combination of a problem $f$ with a given data $x$ is called a *problem instance*, or simply problem unless no confusion occurs.\n",
    "    \n",
    "* A *well-conditioned* problem (instance) is one such that all small perturbations of $x$ lead to only small changes in $f(x)$.\n",
    "\n",
    "* An *ill-conditioned* problem is one such that some small perturbation of $x$ leads to a large change in $f(x)$.\n",
    "\n",
    "* The (relative) *condition number* $\\kappa=\\kappa(x)$ of a problem is defined by\n",
    "$$\n",
    "    \\kappa = \\lim_{\\delta\\to 0}\\sup_{\\|\\delta x\\|\\le \\delta}\\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "    = \\sup_{\\delta x} \\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "$$\n",
    "\n",
    "* For the problem of solving $Ax=b$ for fixed $b$,  $f: A \\mapsto A^{-1}b$, it can be shown that the condition number of $f$ is\n",
    "$$\n",
    "    \\kappa = \\|A\\|\\|A^{-1}\\| =: \\kappa(A)\n",
    "    ,\n",
    "$$\n",
    "where $\\|A\\|$ is the matrix norm induced by vector norm $\\|\\cdot\\|$, i.e.,\n",
    "$$\n",
    "    \\|A\\| = \\sup_{x\\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}.\n",
    "$$\n",
    "If 2-norm is used, then \n",
    "$$\n",
    "\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A),\n",
    "$$\n",
    "the ratio of the maximum and minimum singular values of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above problem, the condition number is matrix $A$ (w.r.t. Euclidean norm) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "* Section II.2, [Computational Statistics](https://link.springer.com/book/10.1007%2F978-0-387-98144-4) by James Gentle (2009).\n",
    "\n",
    "* Sections 1.5 and 2.2, [Applied Numerical Linear Algebra](https://doi.org/10.1137/1.9781611971446) by James W. Demmel (1997).\n",
    "\n",
    "* [What every computer scientist should know about floating-point arithmetic](https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf) by David Goldberg (1991)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "250px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
