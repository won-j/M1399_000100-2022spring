{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dig into the internal representation of R objects\n",
    "library(lobstr)  # if not installed, use install.packages('lobstr')\n",
    "# For unsigned integers\n",
    "library(uint8) # devtools::install_github('coolbutuseless/uint8')\n",
    "# For bitstrings\n",
    "library(pryr)\n",
    "# For big integers\n",
    "library(gmp)\n",
    "# For single precision floating point numbers\n",
    "library(float)\n",
    "library(Rcpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer arithmetics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units of computer storage\n",
    "\n",
    "Humans use decimal digits (why?)\\\n",
    "Computers use binary digits (why?)\n",
    "\n",
    "* *Bit* = binary digit (coined by statistician [John Tukey](https://en.wikipedia.org/wiki/Bit#History)).  \n",
    "* *byte* = 8 bits.\n",
    "* KB = kilobyte = $10^3$ bytes; KiB = kibibyte = $2^{10}$ bytes.  \n",
    "* MB = megabyte = $10^6$ bytes; MiB = mebibyte = $2^{20}$ bytes.\n",
    "* GB = gigabyte = $10^9$ bytes. Typical RAM size.  \n",
    "* TB = terabyte = $10^{12}$ bytes. Typical hard drive size. Size of NYSE each trading session.    \n",
    "* PB = petabyte = $10^{15}$ bytes.  \n",
    "* EB = exabyte = $10^{18}$ bytes. Size of all healthcare data in 2011 is ~150 EB.    \n",
    "* ZB = zetabyte = $10^{21}$ bytes. \n",
    "\n",
    "R function `lobstr::obj_size()` shows the amount of memory (in bytes) used by an object. (This is a better version of the built-in `utils::object.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- 100\n",
    "lobstr::obj_size(x)\n",
    "y <-c(20, 30)\n",
    "lobstr::obj_size(y)\n",
    "z <- as.matrix(runif(100 * 100), nrow=100)  # 100 x 100 random matrix\n",
    "lobstr::obj_size(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all variables in workspace and their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(ls(), function(z, env=parent.env(environment())) obj_size(get(z, envir=env)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage of Characters\n",
    "\n",
    "* Plain text files are stored in the form of characters: `.jl`, `.r`, `.c`, `.cpp`, `.ipynb`, `.html`, `.tex`, ...  \n",
    "* ASCII (American Code for Information Interchange): 7 bits, only $2^7=128$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 0, 1, ..., 127 and corresponding ascii character\n",
    "sapply(0:127, intToUtf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extended ASCII: 8 bits, $2^8=256$ characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers 128, 129, ..., 255 and corresponding extended ascii character\n",
    "sapply(128:255, intToUtf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unicode: UTF-8, UTF-16 and UTF-32 support many more characters including foreign characters; last 7 digits conform to ASCII. \n",
    "\n",
    "* [UTF-8](https://en.wikipedia.org/wiki/UTF-8) is the current dominant character encoding on internet.  \n",
    "\n",
    "<img src=\"./unicode.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "Source: [Google Blog](https://googleblog.blogspot.com/2012/02/unicode-over-60-percent-of-web.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st <-  \"\\uD1B5\\uACC4\\uACC4\\uC0B0\"\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integers: fixed-point number system\n",
    "\n",
    "* Fixed-point number system is a computer model for integers $\\mathbb{Z}$. \n",
    "    - **Remember** that computer memory is finite whereas the cardinality of $\\mathbb{Z}$ is (countably) infinite.\n",
    "    - *Any* representation of numbers in computer *has to be* an approximation.\n",
    "\n",
    "* The number $M$ of bits and method of representing negative numbers vary from system to system. \n",
    "    - The `integer` type in R has $M=32$ (packages such as ‘bit64’support 64 bit integers). \n",
    "        + <https://www.r-bloggers.com/r-in-a-64-bit-world/>\n",
    "    - C has (`unsigned`) `char`, `int`, `short`, `long` (and `long long`), whose sizes depend on the machine.\n",
    "    - Matlab has `(u)int8`, `(u)int16`, `(u)int32`, `(u)int64`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsigned integers\n",
    "\n",
    "* Model for $\\mathbb{N} \\cup \\{0\\}$.\n",
    "* For unsigned integers, the range is $[0,2^M-1]$.\n",
    "* R does not support unsigned integers natively (will see `uint8` package later). In most other languages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Type        | Min           | Max  |\n",
    "| ------------- |:-------------:|:------|\n",
    "|UInt8\t|0\t|255|\n",
    "|UInt16\t|0\t|65535|\n",
    "|UInt32\t|0\t|4294967295|\n",
    "|UInt64\t|0\t|18446744073709551615|\n",
    "|UInt128\t|0\t|340282366920938463463374607431768211455|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signed integers\n",
    "\n",
    "* Model of $\\mathbb{Z}$. Can do subtraction.\n",
    "\n",
    "* First bit (\"most significant bit\" or MSB) is the sign bit.  \n",
    "    - `0` for nonnegative numbers\n",
    "    - `1` for negative numbers  \n",
    "    \n",
    "* **Two's complement representation** for negative numbers. \n",
    "    - Set the sign bit to 1  \n",
    "    - Negate (`0`->`1`, `1`->`0`) the remaining bits\n",
    "    - Add to `1` to the result  \n",
    "    - Two's complement representation of a negative integer $x$ is the same as the unsigned integer $2^M - x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class(5L)  # postfix `L` means integer in R\n",
    "pryr::bits(5L)\n",
    "pryr::bits(-5L)\n",
    "pryr::bits(2L * 5L) # shift bits of 5 to the left (why?)\n",
    "pryr::bits(2L * -5L); # shift bits of -5 to left "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Two's complement representation respects modular arithmetic nicely.  \n",
    "    Addition of any two signed integers are just bitwise addition, possibly modulo $2^M$\n",
    "    - $M=4$ case:\n",
    "    \n",
    "<img src=\"http://users.dickinson.edu/~braught/courses/cs251f02/classes/images/twosCompWheel.png\" width=\"400\" align=\"center\"/>    \n",
    "\n",
    "Source: [Signed Binary Numbers, Subtraction and Overflow](http://users.dickinson.edu/~braught/courses/cs251f02/classes/notes07.html) by Grant Braught"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Range** of representable integers by $M$-bit **signed integer** is $[-2^{M-1},2^{M-1}-1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine$integer.max  # R uses 32-bit integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most other languages,\n",
    "\n",
    "| Type        | Min           | Max  |\n",
    "| ------------- |--------------:|:------|\n",
    "|Int8\t|\t-128|127|\n",
    "|Int16\t|-32768\t|32767|\n",
    "|Int32\t|-2147483648\t|2147483647|\n",
    "|Int64\t|-9223372036854775808\t|9223372036854775807|\n",
    "|Int128\t|-170141183460469231731687303715884105728\t|170141183460469231731687303715884105727|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow for integer arithmetic\n",
    "\n",
    "R reports `NA` for integer overflow and underflow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The largest integer R can hold\n",
    ".Machine$integer.max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M <- 32\n",
    "big <- 2^(M-1) - 1\n",
    "as.integer(big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine$integer.max + 1L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unit8` outputs the result according to modular arithmetic. So does C and Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uint8::as.uint8(255) + uint8::as.uint8(1)\n",
    "uint8::as.uint8(250) + uint8::as.uint8(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package `gmp` supports big integers with arbitrary precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp::as.bigz(.Machine$integer.max ) + gmp::as.bigz(1L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real numbers: floating-point number system\n",
    "\n",
    "Floating-point number system is a computer model for the real line $\\mathbb{R}$.\n",
    "\n",
    "* Most computer systems adopt the [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point), established in 1985, for floating-point arithmetics.  \n",
    "For the history, see an [interview with William Kahan](http://www.cs.berkeley.edu/~wkahan/ieee754status/754story.html).\n",
    "\n",
    "* In the scientific notation, a real number is represented as\n",
    "$$\n",
    "\\pm d_1.d_2d_3 \\cdots d_p \\times b^e, \\quad 0 \\le d_i < b.\n",
    "$$\n",
    "Humans use the _base_ $b=10$ and _digits_ $d_i=0, 1, \\dotsc, 9$.\\\n",
    "    In computer, the base is $b=2$ and the digits $d_i$ are 0 or 1.\n",
    "\n",
    "* **Normalized** vs **denormalized** numbers. For example, decimal number 18 is\n",
    "$$ +1.0010 \\times 2^4 \\quad (\\text{normalized})$$\n",
    "or, equivalently,\n",
    "$$ +0.1001 \\times 2^5 \\quad (\\text{denormalized}).$$\n",
    "\n",
    "* In the floating-number system, computer stores \n",
    "    - sign bit  \n",
    "    - the _fraction_ (or _mantissa_, or _significand_) of the **normalized** representation\n",
    "    - the actual exponent _plus_ a bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* R supports *double precesion* floating point numbers (see below) via `double`. \n",
    "\n",
    "* C supports floating point types `float` and `double`, where in most systems `float` corresponds to single precision while `double` corresponds to double precision.\n",
    "\n",
    "* Julia provides `Float16` (half precision, implemented in software using `Float32`), `Float32` (single precision), `Float64` (double precision), and `BigFloat` (arbitrary precision).\n",
    "\n",
    "> R has no single precision data type. All real numbers are stored in double precision format. The functions `as.single` and `single` are identical to `as.double` and `double` except they set the attribute `Csingle` that is used in the `.C` and `.Fortran` interface, and they are intended only to be used in that context. [R Documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/double.html)\n",
    "\n",
    "* For ease of exposition, we begin with half precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half precision (Float16)\n",
    "\n",
    "<img src=\"./half-precision-numbers.png\" width=\"300\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Half-precision_floating-point_format>\n",
    "    \n",
    "- In Julia, `Float16` is the type for half precision numbers.\n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 10 significand bits (**fraction**=**mantissa**), hence $p=11$ (why?)\n",
    "\n",
    "- 5 exponent bits: $e_{\\max}=15$, $e_{\\min}=-14$, **bias**=15 = $01111_2$ for encoding:\n",
    "    + $e_{\\min} = \\mathbf{00001_2} - 01111_2 = -14_{10}$\n",
    "    + $e_{\\max} = \\mathbf{11110_2} - 01111_2 = 15_{10}$\n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 4}$ in decimal because $\\log_{10} (2^{15}) \\approx 4$.  \n",
    "\n",
    "- **Precision**: $\\log_{10}2^{11} \\approx 3.311$ decimal digits. \n",
    "\n",
    "$$\n",
    "(value) = (-1)^{b_{15}}\\times 2^{(\\sum_{j=1}^5 b_{15-j}2^{5-j}) - 15} \\times \\left( 1 + \\sum_{i=1}^{10}\\frac{b_{10-i}}{2^i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Julia\n",
    "# This is Julia\n",
    "println(\"Half precision:\")\n",
    "@show bitstring(Float16(5)) # 5 in half precision\n",
    "@show bitstring(Float16(-5)); # -5 in half precision\n",
    "```\n",
    "\n",
    "```\n",
    "Half precision:\n",
    "bitstring(Float16(5)) = \"0100010100000000\"\n",
    "bitstring(Float16(-5)) = \"1100010100000000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single precision (Float32, or `float`)\n",
    "\n",
    "<img src=\"./single-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Single-precision_floating-point_format>\n",
    "\n",
    "- In C, `float` is the type for single precision numbers for most systems.\n",
    "- In Julia, `Float32` is the type for single precision numbers.  \n",
    "- In R, single precision is not supported natively. We use the following workaround:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework: figure out how this C++ code works\n",
    "Rcpp::cppFunction('int float32bin(double x) {\n",
    "    float flx = (float) x; \n",
    "    unsigned int binx = *((unsigned int*)&flx); \n",
    "    return binx; \n",
    "}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSB is the sign bit.  \n",
    "\n",
    "- 23 significand bits ($p=24$).  \n",
    "\n",
    "- 8 exponent bits: $e_{\\max}=127$, $e_{\\min}=-126$, **bias**=127.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 38}$ in decimal because $\\log_{10} (2^{127}) \\approx 38$.\n",
    "\n",
    "- **precision**: $\\log_{10}(2^{24}) \\approx 7.225$ decimal digits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message(\"Single precision:\")\n",
    "pryr::bits(float32bin(5)) # 5 in single precision\n",
    "pryr::bits(float32bin(-5)) # -5 in single precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double precision (Float64, or `double`)\n",
    "\n",
    "<img src=\"./double-precision-numbers.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "Source: <https://en.wikipedia.org/wiki/Double-precision_floating-point_format>\n",
    "\n",
    "- Double precision (64 bits = 8 bytes) numbers are the dominant data type in scientific computing.\n",
    "\n",
    "- In C, `double` is the type for double precision numbers for most systems. It is the **default** type for `numeric` values.\n",
    "\n",
    "- In Julia, `Float64` is the type for double precision numbers.    \n",
    "\n",
    "- In R, `double` is the type for double precision numbers.    \n",
    "\n",
    "- MSB is the sign bit.  \n",
    "\n",
    "- 52 significand bits ($p=53$).\n",
    "\n",
    "- 11 exponent bits: $e_{\\max}=1023$, $e_{\\min}=-1022$, **bias**=1023.  \n",
    "\n",
    "- $e_{\\text{min}}-1$ and $e_{\\text{max}}+1$ are reserved for special numbers.  \n",
    "\n",
    "- range of **magnitude**: $10^{\\pm 308}$ in decimal because $\\log_{10} (2^{1023}) \\approx 308$.  \n",
    "\n",
    "- **precision** to the $\\log_{10}(2^{53}) \\approx 15.95$ decimal point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message(\"Double precision:\")\n",
    "pryr::bits(5)    # 5 in double precision\n",
    "pryr::bits(-5)   # -5 in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special floating-point numbers\n",
    "\n",
    "- Exponent $e_{\\max}+1$ plus a zero mantissa means $\\pm \\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(Inf)    # Inf in double precision\n",
    "pryr::bits(-Inf)   # -Inf in double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\max}+1$ plus a nonzero mantissa means `NaN`. `NaN` could be produced from `0 / 0`, `0 * Inf`, ...  \n",
    "\n",
    "- In general `NaN ≠ NaN` bitwise. Test whether a number is `NaN` by `is.nan` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0 / 0)  # NaN\n",
    "pryr::bits(0 * Inf) # NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a zero mantissa represents the real number 0 (\"exact zero\").  \n",
    "    + Why do we need an exact zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0.0)  # 0 in double precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponent $e_{\\min}-1$ with a nonzero mantissa are for numbers less than $b^{e_{\\min}}$.  \n",
    "    Numbers are _denormalized_ in the range $(0,b^{e_{\\min}})$ -- **gradual underflow**. \n",
    "- For example, in half-precision, $e_{\\min}=-14$ but $2^{-24}$ is represented by $0.0000000001_2 \\times 2^{-14}$.\n",
    "- In single precision, $e_{\\min}=-126$ but $2^{-149}$ is represented by $0.00000000000000000000001_2 \\times 2^{-126}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2^(-126)  # emin=-126\n",
    "pryr::bits(float32bin(2^(-126)))\n",
    "2^(-149)  # denormalized\n",
    "pryr::bits(float32bin(2^(-149)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding\n",
    "\n",
    "* Rounding is necessary whenever a number has more than $p$ significand bits. Most computer systems use the default IEEE 754 _round to nearest, ties to even_ mode: \n",
    "\n",
    "* *Round to nearest*: for example, the number 1/10 cannot be represented accurately as a (binary) floating point number:\n",
    "$$ 0.1 = 1.10011001\\dotsc_2 \\times 2^{-4} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pryr::bits(0.1)  # double precision \n",
    "pryr::bits(float32bin(0.1)) # single precision, 1001 gets rounded to 101(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In double precision, \n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 ...`\n",
    "\n",
    "must be rounded to either\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001`\n",
    "\n",
    "or \n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1010`.\n",
    "\n",
    "The midway of these two numbers is\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1000 0...`\n",
    "\n",
    "hence $0.1_{10}$ is closer to `1.1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1001 1010`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In single precision, the number\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 1001 ...`\n",
    "\n",
    "falls between\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 100`\n",
    "\n",
    "and\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 101`.\n",
    "\n",
    "The midway between these two numbers is\n",
    "\n",
    "`1.1001 1001 1001 1001 1001 1001 0000 0...`\n",
    "\n",
    "Hence $0.1_{10}$ is closer to `1.1001 1001 1001 1001 1001 101`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Ties to even*: if the number falls midway, it is rounded to the nearest value with an even least significant digit.\n",
    "\n",
    "* For example, consider the case that the precision is 4 binary digits\n",
    "\n",
    "\n",
    "Exact number | Rounded value | Remainder bits |\n",
    "-------------|---------------|----------------|\n",
    "1.000011*2^1 | 1.000*2^1     | 011 -> round down\n",
    "1.000110*2^1 | 1.001*2^1     | 110 -> round up\n",
    "1.011100*2^1 | 1.100*2^1     | 100 -> round up (*)\n",
    "1.010100*2^1 | 1.010*2^1     | 100 -> round down (**)\n",
    "\n",
    "\n",
    "- In the third example (*), `1.011 100` is precisely midway between `1.011` and `1.100`. The ties to even rule chooses `1.100`, i.e., the representation whose the least significant bit is zero.\n",
    "\n",
    "- In the fourth example (**), `1.010 100` is also precisely midway between `1.010` and `1.011`. The ties to even rule now chooses to round down to `1.010`.\n",
    "\n",
    "- To summarize, if the remainder bits below the precision are `1000 0...`, then the ties to even rule applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors\n",
    "\n",
    "Rounding (more fundamentally, finite precision) incurs errors in floating porint computation. If a real number $x$ is represented by a floating point number $[x]$, then\n",
    "\n",
    "* Absolute error\n",
    "$$\n",
    "    | [x] - x |\n",
    "$$\n",
    "\n",
    "* Relative error\n",
    "$$\n",
    "    \\frac{| [x] - x |}{|x|}\n",
    "$$\n",
    "(if $x \\neq 0$).\n",
    "\n",
    "Of course, we want to ensure that the error after a computation is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine epsilons\n",
    "\n",
    "- Floating-point numbers do not occur uniformly over the real number line\n",
    "    <img src=\"http://www.volkerschatz.com/science/pics/fltscale-wh.png\" width=\"700\" align=\"center\"/>\n",
    "    \n",
    "    Source: [What you never wanted to know about floating point but will be forced to find out](http://www.volkerschatz.com/science/float.html)\n",
    "    \n",
    "- Same number of representible numbers in $(2^i, 2^{i+1}]$ as in $(2^{i+1}, 2^{i+2}]$. Within an interval, they are uniformly distributed.\n",
    "    \n",
    "- **Machine epsilons** are the spacings of numbers around 1: \n",
    "    + $\\epsilon_{\\max}$ = (smallest positive floating point number that added to 1 will give a result different from 1) = $\\frac{1}{2^p} + \\frac{1}{2^{2p-1}}$\n",
    "    + $\\epsilon_{\\min}$ = (smallest positive floating point number that subtracted from 1 will give a result different from 1) = $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$.\n",
    "    \n",
    "    <img src=\"./machine_epsilons.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "Source: *Computational Statistics*, James Gentle, Springer, New York, 2009.\n",
    "    \n",
    "- \"*Round to nearest, ties to even*\" rule: rounds to the nearest value; if the number falls midway, it is rounded to the nearest value with an even least significant digit (default for IEEE 754 binary floating point numbers)\n",
    "\n",
    "- Any real number in the interval $\\left[1 - \\frac{1}{2^{p+1}}, 1 + \\frac{1}{2^p}\\right]$ is represented by a floating point number $1 = 1.00\\dotsc 0_2 \\times 2^0$.\n",
    "\n",
    "- Adding $\\frac{1}{2^p}$ to 1 won't reach the next representable floating point number  $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$. Hence $\\epsilon_{\\max} > \\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$.\n",
    "\n",
    "- Adding the floating point number next to $\\frac{1}{2^p} = 1.00\\dotsc 0_2 \\times 2^{-p}$ to 1 WILL result in $1.00\\dotsc 01_2 \\times 2^0 = 1 + \\frac{1}{2^{p-1}}$, hence $\\epsilon_{\\max} = 1.00\\dotsc 01_2 \\times 2^{-p} = \\frac{1}{2^p} + \\frac{1}{2^{p+p-1}}$.\n",
    "\n",
    "- Subtracting $\\frac{1}{2^{p+1}}$ from 1 results in $1-\\frac{1}{2^{p+1}} = \\frac{1}{2} + \\frac{1}{2^2} + \\dotsb + \\frac{1}{2^p} + \\frac{1}{2^{p+1}}$, which is represented by the floating point number $1.00\\dotsc 0_2 \\times 2^{0} = 1$ by the \"ties to even\" rule. Hence $\\epsilon_{\\min} > \\frac{1}{2^{p+1}}$.\n",
    "\n",
    "- The smallest positive floating point number larger than $\\frac{1}{2^{p+1}}$ is $\\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}=1.00\\dotsc 1_2 \\times 2^{-p-1}$. Thus $\\epsilon_{\\min} = \\frac{1}{2^{p+1}} + \\frac{1}{2^{2p}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine precision\n",
    "\n",
    "* Machine epsilon is often called the machine precision.\n",
    "\n",
    "* If a positive real number $x \\in \\mathbb{R}$ is represented by $[x]$ in the floating point arithmetic, then \n",
    "$$\n",
    "    [x] = \\left( 1 + \\sum_{i=1}^{p-1}\\frac{b_{i+1}}{2^i}\\right) \\times 2^e.\n",
    "$$\n",
    "Thus $x - \\frac{2^e}{2^p} < [x] \\le x + \\frac{2^e}{2^p}$, \n",
    "and\n",
    "$$\n",
    "    \\begin{split}\n",
    "    \\frac{| x - [x] |}{|x|} &\\le \\frac{2^e}{2^p|x|} \\le \\frac{2^e}{2^p}\\frac{1}{[x]-2^e/2^p} \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e(1-1/2^p)}  \\quad (\\because [x] \\ge 2^e) \\\\\n",
    "                            &\\le \\frac{2^e}{2^p}\\frac{1}{2^e}(1 + \\frac{1}{2^{p-1}}) \\\\\n",
    "                            &= \\frac{1}{2^p} + \\frac{1}{2^{2p-1}} = \\epsilon_{\\max}.\n",
    "    \\end{split}\n",
    "$$\n",
    "That is, the **relative error** of the floating point representation $[x]$ of real number $x$ is bounded by $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(digits=20)\n",
    "\n",
    "print(2^(-53) + 2^(-105))   # epsilon_max for double\n",
    "print(1.0 + 2^(-53))\n",
    "print(1.0 + (2^(-53) + 2^(-105)))\n",
    "print(1.0 + 2^(-53) + 2^(-105))  # why is the result?  See \"Catastrophic cancellation\"\n",
    "\n",
    "\n",
    "print(as.double(float::fl(2^(-24) + 2^(-47))))  # epsilon_max for float\n",
    "print(as.double(float::fl(1.0) + float::fl(2^(-24))))\n",
    "print(as.double(float::fl(1.0) + float::fl(2^(-24) + 2^(-47))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/2^54 + 1/2^106)  # epsilon_min for double\n",
    "print(1 - (1/2^54 + 1/2^106))\n",
    "pryr::bits(1.0)\n",
    "pryr::bits(1 - (1/2^54 + 1/2^106))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In R, the variable `.Machine` contains numerical characteristics of the machine. `double.neg.eps` is our $\\epsilon_{\\max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point arithmetic\n",
    "\n",
    "The IEEE 754 standard guarantees that the arithmetic operations involving floating point numbers is so that the operation result in the floating point representation of the exact arithmetric of the involved floating point numbers. For example, suppose $x$ and $y$ are two exact real numbers. Let $[x]$ and $[y]$ be their floating point representation, after the appropriate rounding. Then, the addition of these two numbers is computed like the following.\n",
    "$$\n",
    "\\begin{split}\n",
    "    z &= [x] + [y] \\quad \\text{(exact arithmetric)}\n",
    "    \\\\\n",
    "    z &\\gets [z] \\quad \\text{(rounding)}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overflow and underflow of floating-point number\n",
    "\n",
    "* For double precision, the range is $\\pm 10^{\\pm 308}$. In most situations, underflow (magnitude of result is less than $10^{-308}$) is preferred over overflow (magnitude of result is larger than $10^{+308}$). Overflow produces $\\pm \\inf$. Underflow yields zeros or denormalized numbers. \n",
    "\n",
    "* E.g., the logit link function is\n",
    "$$p = \\frac{\\exp (x^T \\beta)}{1 + \\exp (x^T \\beta)} = \\frac{1}{1+\\exp(- x^T \\beta)}.$$\n",
    "The former expression can easily lead to `Inf / Inf = NaN`, while the latter expression leads to gradual underflow.\n",
    "\n",
    "* `.Machine$double.xmax` and `.Machine$double.xmin` functions gives largest and smallest _non-subnormal_ number represented by the given floating point type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catastrophic cancellation\n",
    "\n",
    "* **Scenario 1**: Addition or subtraction of two numbers of widely different magnitudes: $a+b$ or $a-b$ where $a \\gg b$ or $a \\ll b$. We lose the precision in the number of smaller magnitude. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxx ... \\times 2^{30} \\\\  \n",
    "    b &=& y.yyy... \\times 2^{-30}\n",
    "\\end{eqnarray*}$$\n",
    "What happens when computer calculates $a+b$? We get $a+b=a$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a <- 2.0^30)\n",
    "(b <- 2.0^-30)\n",
    "(a + b == a)\n",
    "pryr::bits(a)\n",
    "pryr::bits(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: suppose we want to compute $x + y$, $x, y > 0$. Let the relative error of $x$ and $y$ be\n",
    "$$\n",
    "\\delta_x = \\frac{[x] - x}{x},\n",
    "\\quad\n",
    "\\delta_y = \\frac{[y] - y}{y}\n",
    ".\n",
    "$$\n",
    "What the computer actually calculates is $[x] + [y]$, which in turn is represented by $[ [x] + [y] ]$. The relative error of this representation is\n",
    "$$\n",
    "\\delta_{\\text{sum}} = \\frac{[[x]+[y]] - ([x]+[y])}{[x]+[y]}\n",
    ".\n",
    "$$\n",
    "Recall that $|\\delta_x|, |\\delta_y|, |\\delta_{\\text{sum}}| \\le \\epsilon_{\\max}$.\n",
    "\n",
    "We want to find a bound of the relative error of $[[x]+[y]]$ with respect to $x+y$.\n",
    "Since $|[x]+[y]| = |x(1+\\delta_x) + y(1+\\delta_y)| \\le |x+y|(1+\\epsilon_{\\max})$, we have\n",
    "$$\n",
    "\\begin{split}\n",
    "| [[x]+[y]-(x+y) | &= | [[x]+[y]] - [x] - [y] + [x] - x + [y] - y | \\\\\n",
    "                   &\\le | [[x]+[y]] - [x] - [y] | +  | [x] - x | + | [y] - y | \\\\\n",
    "                   &\\le |\\delta_{\\text{sum}}([x]+[y])| + |\\delta_x x| + |\\delta_y y| \\\\\n",
    "                   &\\le \\epsilon_{\\max}(x+y)(1+\\epsilon_{\\max}) + \\epsilon_{\\max}x + \\epsilon_{\\max}y \\\\\n",
    "                   &\\approx 2\\epsilon_{\\max}(x+y)\n",
    "\\end{split}\n",
    "$$\n",
    "because $\\epsilon_{\\max}^2 \\approx 0$. Thus\n",
    "$$\n",
    "\\frac{| [[x]+[y]-(x+y) |}{|x+y|} \\le 2\\epsilon_{\\max}\n",
    "$$\n",
    "approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Scenario 2**: Subtraction of two nearly equal numbers eliminates significant digits.  $a-b$ where $a \\approx b$. Consider \n",
    "$$\\begin{eqnarray*}\n",
    "    a &=& x.xxxxxxxxxx1ssss  \\\\\n",
    "    b &=& x.xxxxxxxxxx0tttt\n",
    "\\end{eqnarray*}$$\n",
    "The result is $1.vvvvu...u$ where $u$ are unassigned digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olddigits <- options('digits')$digits\n",
    "options(digits=20)\n",
    "\n",
    "a <- float::fl(1.2345678) # rounding\n",
    "pryr::bits(float32bin(as.double(a))) # rounding\n",
    "b <- float::fl(1.2345677)\n",
    "pryr::bits(float32bin(as.double(b)))\n",
    "print(as.double(a - b)) # correct result should be 1f-7\n",
    "pryr::bits(float32bin(as.double(a - b)))   # must be 1.0000...0 x 2^(-23)\n",
    "print(1/2^23)\n",
    "\n",
    "options(digits=olddigits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: Let\n",
    "$$\n",
    "[x] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{1}{2^{p-1}},\n",
    "\\quad\n",
    "[y] = 1 + \\sum_{i=1}^{p-2}\\frac{d_{i+1}}{2^i} + \\frac{0}{2^{p-1}}\n",
    ".\n",
    "$$\n",
    "\n",
    "* $[x]-[y] = \\frac{1}{2^{p-1}} = [[x]-[y]]$.\n",
    "\n",
    "* The true difference $x-y$ may lie anywhere in $(0, \\frac{1}{2^{p-2}}+\\frac{1}{2^{2p}}]$.\n",
    "\n",
    "* Relative error \n",
    "$$\n",
    "\\frac{|x-y -[[x]-[y]]|}{|x-y|}\n",
    "$$\n",
    "is unbounded -- no guarantee of any significant digit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implications for numerical computation\n",
    "    - Rule 1: add small numbers together before adding larger ones  \n",
    "    - Rule 2: add numbers of like magnitude together (paring). When all numbers are of same sign and similar magnitude, add in pairs so each stage the summands are of similar magnitude  \n",
    "    - Rule 3: avoid substraction of two numbers that are nearly equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebraic laws\n",
    "\n",
    "Floating-point numbers may violate many algebraic laws we are familiar with, such associative and distributive laws. See the example in the Machine Epsilon section and HW1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coditioning\n",
    "\n",
    "Condiser solving a linear system $Ax=b$:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "whose solution is $(x_1, x_2) = (1.000, 1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A <- matrix(c(1.0, 0.667, 0.5, 0.333), nrow=2)\n",
    "b <- c(1.5, 1.0)\n",
    "solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we *perturb* $b$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.333 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 0.999 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then the solution changes to $(x_1, x_2) = (0.000, 3.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 <- c(1.5, 0.999)\n",
    "solve(A, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we instead perturb $A$ by 0.001, i.e., solve\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1.000 & 0.500 \\\\ 0.667 & 0.334 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1.500 \\\\ 1.000 \\end{bmatrix}\n",
    ",\n",
    "$$\n",
    "\n",
    "then this time the solution changes to $(x_1, x_2) = (2.000, -1.000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 <- matrix(c(1.0, 0.667, 0.5, 0.334), nrow=2)\n",
    "solve(A1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, an input perturbation of order $10^{-3}$ yield an output perturbation of order $10^0$. Thats 3 orders of magnutides of relative change!\n",
    "\n",
    "Floating point representation $[x]$ of a real number $x$ in a digital computer may introduce such input perturbation easily. The perturbation of output of a problem with respect to the input is called *conditioning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Abstractly, a *problem* can be viewed as function $f: X \\to Y$ where $X$ is a normed vector space of data and $Y$ is a normed vector space of solutions.\n",
    "    - The problem of solving $Ax=b$ for fixed $b$ is $f: A \\mapsto A^{-1}b$ with $X=\\{M\\in\\mathbb{R}^{n\\times n}: M \\text{ is invertible} \\}$ and $Y = \\mathbb{R}^n$.\n",
    "    - The combination of a problem $f$ with a given data $x$ is called a *problem instance*, or simply problem unless no confusion occurs.\n",
    "    \n",
    "* A *well-conditioned* problem (instance) is one such that all small perturbations of $x$ lead to only small changes in $f(x)$.\n",
    "\n",
    "* An *ill-conditioned* problem is one such that some small perturbation of $x$ leads to a large change in $f(x)$.\n",
    "\n",
    "* The (relative) *condition number* $\\kappa=\\kappa(x)$ of a problem is defined by\n",
    "$$\n",
    "    \\kappa = \\lim_{\\delta\\to 0}\\sup_{\\|\\delta x\\|\\le \\delta}\\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "    = \\sup_{\\delta x} \\frac{\\|\\delta f\\|/\\|f(x)\\|}{\\|\\delta x\\|/\\|x\\|}\n",
    "$$\n",
    "\n",
    "* For the problem of solving $Ax=b$ for fixed $b$,  $f: A \\mapsto A^{-1}b$, it can be shown that the condition number of $f$ is\n",
    "$$\n",
    "    \\kappa = \\|A\\|\\|A^{-1}\\| =: \\kappa(A)\n",
    "    ,\n",
    "$$\n",
    "where $\\|A\\|$ is the matrix norm induced by vector norm $\\|\\cdot\\|$, i.e.,\n",
    "$$\n",
    "    \\|A\\| = \\sup_{x\\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}.\n",
    "$$\n",
    "If 2-norm is used, then \n",
    "$$\n",
    "\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A),\n",
    "$$\n",
    "the ratio of the maximum and minimum singular values of $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above problem, the condition number is matrix $A$ (w.r.t. Euclidean norm) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "* Section II.2, [Computational Statistics](https://link.springer.com/book/10.1007%2F978-0-387-98144-4) by James Gentle (2009).\n",
    "\n",
    "* Sections 1.5 and 2.2, [Applied Numerical Linear Algebra](https://doi.org/10.1137/1.9781611971446) by James W. Demmel (1997).\n",
    "\n",
    "* [What every computer scientist should know about floating-point arithmetic](https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf) by David Goldberg (1991)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "250px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
